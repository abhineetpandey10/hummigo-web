# -*- coding: utf-8 -*-
#Author: Varenya Srivastava
"""FRAKE_Keyword_Varenya.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hqo0M3Es0itCg5mM58iAmEVIt5UuBUnl
"""

#imports
import pip
import networkx as nx
import numpy as np
from nltk.tokenize import word_tokenize ,sent_tokenize
from nltk.corpus import stopwords
import pandas as pd
import string
from nltk import pos_tag
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from nltk import pos_tag

#Fp
import itertools
import json
import numpy as np
import copy
import sys

#Hup
from  itertools import chain

def create_initialset(dataset):
    retDict = {}
    midlist = []
    for trans in dataset:
        if trans in midlist:
            retDict[frozenset(trans)] = retDict[frozenset(trans)] +1
        else:
            retDict[frozenset(trans)] = 1
        midlist.append(trans)
    return retDict

def sorter(income):
    for ii in Dataset:
        tmp =[]
        tmp2 =[]
        for i in income:
            if i in ii:
                tmp.append(ii.index(i))
            else:
                break
        tmp.sort()
        for i in tmp:
            tmp2.append(ii[i])
        if len(tmp2) == len(income):
            out = tmp2 
            break
    return out



#class of FP TREE node
class TreeNode:
    def __init__(self, Node_name,counter,parentNode):
        self.name = Node_name
        self.count = counter
        self.nodeLink = None
        self.parent = parentNode
        self.children = {}
        
    def increment_counter(self, counter):
        self.count += counter


#To create Headertable and ordered itemsets for FP Tree
def create_FPTree(dataset, minSupport):
    HeaderTable = {}
    #term frequent of all items
    for transaction in dataset:
        for item in transaction:
            HeaderTable[item] = HeaderTable.get(item,0) + dataset[transaction]
    #delete min item
    for k in list(HeaderTable):
        if HeaderTable[k] < minSupport:
            del(HeaderTable[k])
            
    frequent_itemset = set(HeaderTable.keys())
    #ops not have any item
    if len(frequent_itemset) == 0:
        return None, None
    #add new part to value of headertable 
    for k in HeaderTable:
        HeaderTable[k] = [HeaderTable[k], None]
    #crate null set
    retTree = TreeNode('Null Set',1,None)
    for itemset,count in dataset.items():
        frequent_transaction = {}
        for item in itemset:
            # to apply min support
            if item in frequent_itemset:
                frequent_transaction[item] = HeaderTable[item][0]
        #if didn't com any item from ttemset skip that
        if len(frequent_transaction) > 0:
            #to get ordered itemsets form transactions
            # this part break the order 
            global ordered_itemset
            ordered_itemset = [v[0] for v in sorted(frequent_transaction.items(), key=lambda p: p[1], reverse=True)]
            #to update the FPTree
            updateTree(ordered_itemset, retTree, HeaderTable, count)
    return retTree, HeaderTable


#To create the FP Tree using ordered itemsets
def updateTree(itemset, FPTree, HeaderTable, count):
    if itemset[0] in FPTree.children:
        FPTree.children[itemset[0]].increment_counter(count)
    else:
        FPTree.children[itemset[0]] = TreeNode(itemset[0], count, FPTree)

        if HeaderTable[itemset[0]][1] == None:
            HeaderTable[itemset[0]][1] = FPTree.children[itemset[0]]
        else:
            update_NodeLink(HeaderTable[itemset[0]][1], FPTree.children[itemset[0]])

    if len(itemset) > 1:
        updateTree(itemset[1::], FPTree.children[itemset[0]], HeaderTable, count)

#To update the link of node in FP Tree
def update_NodeLink(Test_Node, Target_Node):
    while (Test_Node.nodeLink != None):
        Test_Node = Test_Node.nodeLink

    Test_Node.nodeLink = Target_Node

#To transverse FPTree in upward direction
def FPTree_uptransveral(leaf_Node, prefixPath):
    if leaf_Node.parent != None:
        prefixPath.append(leaf_Node.name)
        FPTree_uptransveral(leaf_Node.parent, prefixPath)

#To find conditional Pattern Bases
def find_prefix_path(basePat, TreeNode):
    Conditional_patterns_base = {}

    while TreeNode != None:
        prefixPath = []
        FPTree_uptransveral(TreeNode, prefixPath)
        if len(prefixPath) > 1:
            Conditional_patterns_base[frozenset(prefixPath[1:])] = TreeNode.count
        TreeNode = TreeNode.nodeLink
    return Conditional_patterns_base

def Mine_Tree(FPTree, HeaderTable, minSupport, prefix, frequent_itemset):
    if HeaderTable == None:
        bigL = []
    else:
        bigL = [v[0] for v in sorted(HeaderTable.items(),key=lambda p: p[1][0])]
    
    for basePat in bigL:
        new_frequentset = prefix.copy()
        new_frequentset.add(basePat)
        #add frequent itemset to final list of frequent itemsets
        sort_new_frequentset = sorter(new_frequentset)
        if len(new_frequentset) > 1:
            frequent_itemset.update({tuple(sort_new_frequentset):max([HeaderTable[w][1].count for w in list(HeaderTable.keys())])})

        #get all conditional pattern bases for item or itemsets
        Conditional_pattern_bases = find_prefix_path(basePat, HeaderTable[basePat][1])
        

        #call FP Tree construction to make conditional FP Tree
        Conditional_FPTree, Conditional_header = create_FPTree(Conditional_pattern_bases,minSupport)

        if Conditional_header != None:
            Mine_Tree(Conditional_FPTree, Conditional_header, minSupport, new_frequentset, frequent_itemset)


def Fp_growth(dataset , min_Support):                

    
    
    global Dataset
    Dataset = dataset
    initSet = create_initialset(dataset)
    FPtree, HeaderTable = create_FPTree(initSet, min_Support)
    frequent_itemset = dict()
    #call function to mine all ferquent itemsets
    Mine_Tree(FPtree, HeaderTable, min_Support, set([]), frequent_itemset)
    return frequent_itemset

def myfrozenset(alllist):
    unique_list = []

    # traverse for all elements
    for x in alllist:
    # check if exists in unique_list or not \n",
        if x not in unique_list:
            unique_list.append(x) 
    return tuple(unique_list)


    
def HUP(Fp,data,sigma):

    wordfreq = []

    tmp = list(chain.from_iterable(list(Fp.keys())))

    for w in (myfrozenset(tmp)):
        wordfreq.append(list(chain.from_iterable(data)).count(w))  
    utility =  dict(list(zip(myfrozenset(tmp), wordfreq))) #key is unic word and value is word frequece

    def U(item):#Definition 2
        sum_ = 0
        if item == '':
            return(sum_)
        tmp = list(item)
        for j in tmp:
            sum_ += utility[j]
        return(sum_)

    def lists_overlap(I1, I2):
        I1 = list(I1)
        I2 = list(I2)
        tmp = []
        for i in I1:
            if i in I2:
                tmp.append(i)
        return tmp

    def O(item1,item2):
        return U(lists_overlap(item1,item2)) / (U(item1) + U(item2) - U(lists_overlap(item1,item2)))

    P = dict()
    deleted_query = []
    K = 200#len(Fp)
    for q,q_ in (Fp.items()):
        if len(P) <= K:
            #calculate pattern utility
            uq = U(q)
            sortP = sorted(P, key=P.get ,reverse=True)#######
            count = 0
            for p in sortP:
                #calculate overlap-degree
                o_p_q = O(p,q)
                if o_p_q > sigma:
                    if uq > P[p]:
                        deleted_query.append(p)
                        count += 1
                    else:
                        break
                else:
                    count += 1
            if count == len(P):
                P.update({q:uq})
                for itm in deleted_query:
                    del P[itm]
            deleted_query = []
        else:
            break
    return P


class KeywordExtractor :
    def __init__(self,lang,hu_hiper = 0.4,Number_of_keywords=10):
        self.lang = lang
        self.hu_hiper = hu_hiper
        self.Number_of_keywords = Number_of_keywords
        self.Stopwords = {
'en' : ['somehow', 'which', 'before', 'three', 'or','\'ve', 'should', 'might', 'own', 'those', 'to', 'above', 'nor', 'me', 'seems', 'after', 'empty', 'put', 'that', 'will', 'while', 'across', 'been', 'something', 'ie', 'from', 'eight', 'herein', 'below', 'into', 'fifty', 'it', 'when', 'for', 'fifteen', 'top', 'hers', 'anyway', 'between', 'nevertheless', 'the', 'still', 'whither', 'and', 'found', 'our', 'through', 'have', 'whereupon', 'without', 'off', 'am', 'at', 'beside', 'four', 'himself', 'move', 'him', 'be', 'out', 'its', 'thereupon', 'third', 'well', 'yet', 'such', 'themselves', 'as', 'thereafter', 'what', 'whoever', 'sincere', 'until', 'too', 'many', 'not', 'whom', 'again', 'he', 'else', 'latter', 'of', 'on', 'anywhere', 'towards', 'done', 'same', 'side', 'almost', 'find', 'upon', 'everything', 'hundred', 'often', 'thru', 'twenty', 'are', 'afterwards', 'beforehand', 'bottom', 'except', 'ours', 'forty', 'rather', 'either', 'meanwhile', 'since', 'then', 'thereby', 'because', 'once', 'whatever', 'wherein', 'you', 'do', 'everywhere', 'during', 'front', 'she', 'detail', 'indeed', 'system', 'thin', 'name', 'his', 'others', 'somewhere', 'now', 'whereafter', 'is', 'whereas', 'around', 'more', 'cannot', 'onto', 'seem', 'whole', 'much', 'very', 'cry', 'hasnt', 'any', 'sometime', 'alone', 'etc', 'my', 'seeming', 'throughout', 'up', 'their', 'anyone', 'can', 'yours', 'thus', 'take', 'nine', 'along', 'itself', 'ten', 'thence', 'there', 'enough', 'further', 'go', 'interest', 'due', 'hereafter', 'few', 'back', 'formerly', 'here', 'nobody', 'only', 'whenever', 'each', 'moreover', 'anyhow', 'how', 'also', 'un', 'amoungst', 'may', 'hereupon', 'otherwise', 'us', 'was', 'give', 'over', 'some', 'under', 'than', 'becoming', 'amongst', 'mine', 'next', 'fill', 'first', 'please', 'so', 'though', 'another', 'beyond', 'perhaps', 'see', 'fire', 'yourself', 'none', 'whence', 'i', 'has', 'yourselves', 'full', 'noone', 'six', 'all', 'being', 'thick', 'least', 'latterly', 'ltd', 'seemed', 'where', 'together', 'eg', 'other', 'show', 'whether', 'herself', 'among', 'therefore', 'in', 'this', 'made', 'although', 'against', 'hereby', 'wherever', 'de', 'five', 'already', 'could', 'two', 'your', 'never', 'eleven', 'most', 'sixty', 'a', 'however', 'one', 'but', 'her', 'if', 'call', 'get', 'sometimes', 'twelve', 'within', 'mill', 'an', 'nowhere', 'must', 'con', 'everyone', 'per', 'these', 'bill', 'keep', 'neither', 'myself', 'serious', 'we', 'whereby', 'nothing', 'always', 'amount', 'becomes', 'namely', 'behind', 'last', 'mostly', 'therein', 'why', 'even', 'couldnt', 'ever', 'became', 'every', 'down', 'about', 'elsewhere', 'ourselves', 'co', 're', 'by', 'who', 'via', 'former', 'several', 'toward', 'both', 'would', 'someone', 'no', 'whose', 'less', 'describe', 'hence', 'anything', 'them', 'cant', 'they', 'inc', 'part', 'had', 'become', 'were', 'besides', 'with'],
}
         
    def __Pos_t(self,word):
        '''
        This function is used for POS Score
        '''
        tag = pos_tag([word])[0][1]
        if 'NN' in tag:
            return 1
        elif 'V' in tag:
            return 0.5
        elif 'jJ' in tag:
            return 0.35
        else:
            return 0.25


    def __calcCentrality(self,G,cnt):
        '''
        For calculating Graph centrality measures
        '''
        cntV = list()
        if cnt == 'deg':
            cntV = list(dict(G.degree).values())
        elif cnt == 'ei':
            cntV = list(nx.eigenvector_centrality_numpy(G).values())
        elif cnt == 'sh':
            cntV = list(nx.constraint(G).values())
        elif cnt == 'pr':
            cntV = list(nx.pagerank_numpy(G).values())
        elif cnt == 'bw':
            cntV = list(nx.betweenness_centrality(G).values())
        elif cnt == 'cl':
            cntV = list(nx.clustering(G).values())
        elif cnt == 'cc':
            cntV = list(nx.closeness_centrality(G).values())
        elif cnt == 'ec':
            cntV = list(nx.eccentricity(G).values())
    
        else:
            raise ValueError('calcCettrality: wrong cnt value or not implemented yet')
        
        return cntV

    def __getPC1(self,mtxFeatures, setCentralities):
        '''
        PC1 Function
        '''
        sc = StandardScaler()
        A = mtxFeatures.loc[:,setCentralities]
        # normalize data
        A = pd.DataFrame(data = sc.fit_transform(A),  columns = list(A))    
        # create the PCA instance
        pca = PCA(n_components=1)
        # fit on data
        pca.fit(A)
        # access values and vectors    
        PC1 = pd.DataFrame(data=pca.components_, columns = list(A))
        return PC1


    def __MCI_PC1(self,G, PC1, N):      
        '''
        calculating Graph measures with generated waiths
        '''
        sc = MinMaxScaler()
        #sc = StandardScaler()
        
        G_Words = pd.DataFrame()
        G_Words['Word'] = list(G.nodes)
        G_Words['MCI'] = np.zeros((len(G.nodes),1))
        for cnt in list(PC1):
            val = pd.DataFrame(self.__calcCentrality(G,cnt))
            # Normalizing the data
            val = sc.fit_transform(val)
            G_Words[cnt] = val
            G_Words['MCI'] += G_Words[cnt]*abs(PC1.loc[0,cnt]) 
        
        keynodes = G_Words.sort_values(by='MCI', ascending=False).loc[:,['Word','MCI']]
        
        if N == -1:
            return keynodes.reset_index(drop=True)
        else: 
            return keynodes.reset_index(drop=True).head(N)



    def __scores(self,g):
        bw = nx.betweenness_centrality(g)
        cc = nx.closeness_centrality(g)
        ei = nx.eigenvector_centrality_numpy(g)
        deg= nx.degree_centrality(g)
        pr = nx.pagerank(g)
        cl = nx.clustering(g)
        ec = nx.eccentricity(g)
        sh = nx.constraint(g)
        
        return bw , cc , ei,deg,pr, cl,ec ,sh
    def __MCI_Centrality(self,tokens):
        tokens = [s.lower() for s in tokens]
        G = nx.Graph()
        for n in set(tokens):
            G.add_node(n)
        for i in range(len(tokens)-2):
            G.add_edge(tokens[i],tokens[i+1])
            G.add_edge(tokens[i],tokens[i+2])
        

        df = pd.DataFrame({'Word' :list(self.__scores(G)[0].keys()),
                        'bw' :list(self.__scores(G)[0].values()),
                        'cc' :list(self.__scores(G)[1].values()),
                        'ei' :list(self.__scores(G)[2].values()),
                        'deg' :list(self.__scores(G)[3].values()),
                        'pr' :list(self.__scores(G)[4].values()),
                        'cl' :list(self.__scores(G)[5].values()),
                        'ec' :list(self.__scores(G)[6].values()),
                        'sh' :list(self.__scores(G)[7].values())})

        setCentralities = ['bw', 'ei', 'cc', 'deg', 'pr', 'cl', 'ec', 'sh']
            
        #PRELOADED MATRIX OF FEATURES (CENTRALITIES) FROM A RESPOSITORY
            
        mtxFeatures = df.drop(['Word'], axis=1) 



        #Number of requested nodes
        N = -1

        PC1 = self.__getPC1(mtxFeatures, setCentralities)
        keywords = self.__MCI_PC1(G, PC1, N)
        c_score = dict()
        for i in range(len(keywords)):
            c_score.update({keywords['Word'][i]:keywords['MCI'][i]})
        return c_score 

    def __posision(self,word,text):
        median = [index for index, value in enumerate(text) if value.lower() == word.lower()]
        return(np.log(np.log(3+(sum(median)/len(median)))))
    def __cast(self,word,text):
        return max(text.count(word.upper()),text.count(word[0].upper()+word[1:].lower()))/ (1+np.log([i.lower()for i in text].count(word.lower())))
        
    def __sentence(self,word,sents):
        return sum(list(map(int,[word.lower() in i.lower() for i in sents])))/ len(sents)

    def __meanTF(self,word,text):
        meanTF = [text.count(i) for i in set(text)]
        return text.count(word) / (sum(meanTF)/len(meanTF))+1

    def __local(self,tokens,sent):
        y_score = dict()
        for word in tokens:
            y_score.update({word.lower():(self.__cast(word,tokens)+self.__sentence(word,sent)+self.__meanTF(word,tokens))+self.__Pos_t(word)/self.__posision(word,tokens)})
            
        return y_score


    def __score(self,text,sent):
        s = dict()
        y = self.__local(text,sent)
        c = self.__MCI_Centrality(text)
        for i in set(list(y.keys())+list(c.keys())):
            if i not in set(list(y.keys())):
                s.update({i:
                0.3 * c[i] 
                })
                continue
            if i not in set(list(c.keys())):
                s.update({i:
                0.3 * y[i] 
                })
                continue
                
            yi = y[i] 
            ci = c[i]
            s.update({i:
                yi * ci 
                })
        return s


    def extract_keywords(self,yourInputText):
    
        if self.lang not in self.Stopwords.keys():
            print("'{}' is not a valid language".format(self.lang))
        text = yourInputText
        data = []

        
        data_snt = sent_tokenize(text)
        data_tmp = []
        token = []
        for i in (data_snt):
            tokens = word_tokenize(str(i))
            tokens = [(w) for w in tokens if w.lower() not in self.Stopwords[self.lang] and w not in string.punctuation and w.isalpha()]
            data_tmp.append(tokens)
            token.extend(tokens)
        data.append(data_tmp)
        z = self.__score(token,data_snt)
        # Frequent pattern section
        Hup = dict()

        for i in range(20,1,-1):
            Fp = Fp_growth(data[0],i)
            if len(Fp) == 0:
                continue
            Hup = HUP(Fp,data[0],self.hu_hiper)
            if len(Hup)>7:
                break

        kewords = dict()
        kewordsp = dict()


        Pattern_L =list(Hup.keys())

        for i in Pattern_L:
            mult = 0
            for j in i:
                if j in list(z.keys()) :
                    mult += z[j]
            mult = mult
            kewordsp.update({' '.join(i):mult})

        kewordstm =({k: (v) for k, v in sorted(kewordsp.items(), key=lambda item: item[1] , reverse= True)})

        kewords.update({k: kewordstm[k] for k in list(kewordstm)[:int(self.hu_hiper*10)]})
        kewords.update(z)

        
        return {k: round(v,2) for k, v in sorted(kewords.items(), key=lambda item: item[1] , reverse= True)[:self.Number_of_keywords]}

import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

text1 = sys.argv[1]

kw=KeywordExtractor(lang='en',hu_hiper=0.4,Number_of_keywords=1)

keywords = kw.extract_keywords(text1)

'''#stemming
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
ps = PorterStemmer()
for w in keywords:
    print(w, " : ", ps.stem(w))
'''
print()
print((keywords))

'''for txt in keywords:
  print(nltk.pos_tag(txt))'''